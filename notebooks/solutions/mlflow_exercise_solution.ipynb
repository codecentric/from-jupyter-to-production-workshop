{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7262d11d0db6945c",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# MLflow Experiment Tracking \n",
    "MLflow tracking is a powerful tool for logging and organizing machine learning experiments. It provides a centralized repository to log parameters, metrics, artifacts, and code versions. Here are some key concepts:\n",
    "\n",
    "- **Experiment**: A named process, typically representing a machine learning workflow, that can contain multiple runs.\n",
    "- **Run**: A single execution of a script or piece of code within an experiment.\n",
    "- **Parameters**: Input values to a run, such as hyperparameters.\n",
    "- **Metrics**: Output values or performance indicators logged during a run.\n",
    "- **Artifacts**: Output files, such as models or plots, logged during a run.\n",
    "\n",
    "By using MLflow, teams can effectively track and reproduce experiments, facilitating collaboration and model reproducibility.\n",
    "\n",
    "## Exercise Overview\n",
    "In this exercise, we'll explore how to leverage MLflow to log and organize metrics, parameters, and artifacts in the context of machine learning workflows. We also look at how trained models can be automatically saved as Mlflow models. These models are available via the Mlflow Registry and can be retrieved from the Model Registry via a reference if required. Last but not least, let's look at how we can also log the datasets used for a run. If we are using DVC, the currently checked out Git commit of the DVC repo could also be logged as a tag. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918f85af9e55a463",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## 1) - Logging Metrics and Parameters with MLflow\n",
    "> *Note:* The tracking server can be reached via the URL `http://localhost:5001`.\n",
    "\n",
    "In this exercise, we will practice using MLflow to log metrics and parameters in a machine learning workflow.\n",
    "We will use the same functions as we used in the dagster ops job exercise just with little adjustments.\n",
    "\n",
    "As in the Dagster exercise, the places in the code where something needs to be added are marked with `#...`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e411c0f24217a4d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Part 1: Create an experiment and start a run\n",
    "Before you can start the exercise, we need to import some packages. The package `mlflow` is particularly important for tracking experiments with Mlflow. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96da20b9-58fe-46f7-90d8-1ea6bb0ca64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from typing import Tuple\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import mlflow\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import (\n",
    "    ConfusionMatrixDisplay,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    precision_score,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "from xgboost.callback import TrainingCallback"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460c55a7-cb15-477e-9690-0363b36cb490",
   "metadata": {},
   "source": [
    "You can also use MLflow without an active MLflow tracking server. In this case, MLflow saves all data in a folder in the current root directory of the project.\n",
    "For our exercise, however, we use a tracking server. \n",
    "\n",
    "Please set the variable `mlflow_tracking_uri` as the tracking URI to be used by MLflow. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f62a6842a61ad6",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "mlflow_tracking_uri = \"http://mlflow:5001\"\n",
    "mlflow.set_tracking_uri(mlflow_tracking_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ced7863-e8f9-4b4a-ada3-e7cca7cf8a3b",
   "metadata": {},
   "source": [
    "Before you can start logging metrics and parameters, you must first create an MLflow experiment. The experiment is unique via its name. If an experiment with the same name is to be created several times, an exception is thrown. This is intercepted directly by the following code block. In the following, we refer to the experiment via the experiment ID (`exp_id`).\n",
    "\n",
    "Please create an experiment with the name `Spotify genre classification`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d30d47a2c89d8b0",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    exp_id = mlflow.create_experiment(\n",
    "        name=\"Spotify genre classification\"\n",
    "    )\n",
    "except mlflow.exceptions.RestException:\n",
    "    exp_id = mlflow.get_experiment_by_name(\n",
    "        name=\"Spotify genre classification\"\n",
    "    ).experiment_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2303a8b5-94f8-46bf-9de2-e456e24d6018",
   "metadata": {},
   "source": [
    "There are several ways in which a run can be started and ended.\n",
    "Please start a run with the previously created experiment ID (`exp_id`) and then end the run again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8289dafc7951ca7",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "_ = mlflow.start_run(experiment_id=exp_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc50d00-5998-4c4a-b935-aac970894018",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6578148-7693-4be0-a4c6-47e9c4c6782f",
   "metadata": {},
   "source": [
    "The `with` statement can be used to start a run that is automatically ended as soon as the content of the with statement has been processed. Please start a run with the with statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee76aef-912f-419e-97b1-a634c3f52c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run(experiment_id=exp_id):\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e6b4d5-7061-4a39-87d7-c8998b739155",
   "metadata": {},
   "source": [
    "Now that we have created an experiment and performed two runs, open the web UI of the [Mlflow tracking server](http://localhost:5001) and take a look at the experiment and the runs. Admittedly, it is still relatively empty at the moment, but this will change shortly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935ce4ee-d86d-4275-b729-6b9d10358a0b",
   "metadata": {},
   "source": [
    "### Part 2: Log parameters and metrics\n",
    "Now that we have familiarized ourselves with the tracking server and starting runs, it is time to log the first parameters and metrics. A parameter can be anything that makes up the current run. This includes, for example, hyperparameters that you want to optimize over time. It can be very helpful to log parameters to compare runs over time and determine the causes of better or worse model performance. \n",
    "\n",
    "Metrics, in turn, help us gain a better understanding of the performance of the models created in the runs. This includes metrics and information collected during or after model training.\n",
    "\n",
    "> *Note*: Please complete the tasks in this part and then run the cells one by one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07eee6d2-93de-4fe3-a9bb-e3775acad20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = mlflow.start_run(experiment_id=exp_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ad581d-a0e7-42ef-a1a7-bc4435d714be",
   "metadata": {},
   "source": [
    "We have identified the size of the test set as an interesting parameter. Please log the size of the test set (`test_size`) as a parameter under the name (`key`) \"Test size\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809fc185-b030-4007-add9-28dfa41b0e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data() -> Tuple[pd.DataFrame, pd.DataFrame, pd.Series, pd.Series, pd.Series]:\n",
    "    data = pd.read_csv(\"./data/genres_standardized.csv\", sep=\";\")\n",
    "    columns = list(data.columns)\n",
    "    columns.remove(\"genre\")\n",
    "    data[\"genre\"] = data[\"genre\"].astype(\"category\")\n",
    "    data[\"target\"] = data[\"genre\"].cat.codes\n",
    "    test_size = 0.2\n",
    "    mlflow.log_param(key=\"Test size\", value=test_size)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        data[columns], data[\"target\"], test_size=test_size\n",
    "    )\n",
    "    return X_train, X_test, y_train, y_test, data[\"genre\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd24269-36c2-4d83-9da0-9e58bd5dcb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_train, input_test, target_train, target_test, target_names = split_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0231203f-9252-445e-9bea-5ceacccc252c",
   "metadata": {},
   "source": [
    "The following `TrainingCallback` complements the code known from the Dagster exercise. This callback is used to log a metric during the training of the classifier. \n",
    "Please log the metric `metric_value` with the key `metric_name`. Set the step parameter of the `log_metric` function to `epoch`.\n",
    "\n",
    "Then execute the code from `Part 2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e6c6ea-6a0a-46f8-b39e-90593981d4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MlflowCallback(TrainingCallback):\n",
    "    def after_iteration(self, model, epoch, evals_log) -> bool:\n",
    "        for data, metric in evals_log.items():\n",
    "            for metric_name, log in metric.items():\n",
    "                metric_value = sum(log) / len(log)\n",
    "                mlflow.log_metric(\n",
    "                    key=metric_name, value=metric_value, step=epoch\n",
    "                )\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2d085d-32a8-43f6-869f-4b88ea82b50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classifier(\n",
    "    input_train: pd.DataFrame, target_train: pd.Series\n",
    ") -> XGBClassifier:\n",
    "    number_of_estimators: int = 100\n",
    "    learning_rate: float = 0.1\n",
    "    max_depth: int = 8\n",
    "    min_child_weight: float = 1.0\n",
    "    gamma: float = 0\n",
    "    number_of_jobs: int = 4\n",
    "\n",
    "    model = XGBClassifier(\n",
    "        learning_rate=learning_rate,\n",
    "        n_estimators=number_of_estimators,\n",
    "        max_depth=max_depth,\n",
    "        min_child_weight=min_child_weight,\n",
    "        gamma=gamma,\n",
    "        n_jobs=number_of_jobs,\n",
    "        callbacks=[MlflowCallback()],\n",
    "    )\n",
    "    model.fit(\n",
    "        input_train, target_train, eval_set=[(input_train, target_train)], verbose=False\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379ad1fe-5a3c-4600-b17b-569c8df9481a",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = train_classifier(input_train, target_train)\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bde9997-bc22-4276-8498-30d95fe46424",
   "metadata": {},
   "source": [
    "Open the web UI of the [Mlflow Tracking Server](http://localhost:5001) again and view the runs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681a553f-adf7-4120-b1f7-dc4f3cdfaeb7",
   "metadata": {},
   "source": [
    "## 2) Log models, artifacts and datasets\n",
    "Now that we can create experiments and have performed the first runs where parameters and metrics are logged, it's time to log more complex data / larger data. This includes models, artifacts and datasets. In general, you can save any type of file as an artifact, including models and datasets. However, there are advantages for saving datasets and models explicitly as models or datasets. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad895415-9c8e-4a01-8c6f-df7ca6bd067f",
   "metadata": {},
   "source": [
    "### Part 1: Log models\n",
    "We start by logging a trained classifier as a model. Mlflow offers a variety of model falvors that can be used to log models as Mlflow models and store them in a registry: \n",
    "\n",
    "* `Python Function (python_function)`\n",
    "* `R Function (crate)`\n",
    "* `H2O (h2o)`\n",
    "* `Keras (keras)`\n",
    "* `MLeap (mleap)`\n",
    "* `PyTorch (pytorch)`\n",
    "* `Scikit-learn (sklearn)`\n",
    "* `Spark MLlib (spark)`\n",
    "* `TensorFlow (tensorflow)`\n",
    "* `ONNX (onnx)`\n",
    "* `MXNet Gluon (gluon)`\n",
    "* `XGBoost (xgboost)`\n",
    "* `LightGBM (lightgbm)`\n",
    "* `CatBoost (catboost)`\n",
    "* `Spacy(spaCy)`\n",
    "* `fastai(fastai)`\n",
    "* `Statsmodels (statsmodels)`\n",
    "* `Prophet (prophet)`\n",
    "* `Pmdarima (pmdarima)`\n",
    "* `OpenAI (openai) (Experimental)`\n",
    "* `LangChain (langchain) (Experimental)`\n",
    "* `John Snow Labs (johnsnowlabs) (Experimental)`\n",
    "* `Diviner (diviner)`\n",
    "* `Transformers (transformers) (Experimental)`\n",
    "* `SentenceTransformers (sentence_transformers) (Experimental)`\n",
    "\n",
    "The classifier that we have trained is an `XGBoostClassifier`. To log our classifier, we use the function `log_model` from the package `mlflow.xgboos`. \n",
    "\n",
    "Please execute the cells from `Part 1` to start a run where the trained classifier is logged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b1cd9b-946e-434a-9fa5-fdd7bf7765d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = mlflow.start_run(experiment_id=exp_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc096be-d76a-4964-b7b3-be51d0569903",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = train_classifier(input_train, target_train)\n",
    "mlflow.xgboost.log_model(classifier, \"spotify_genre_classifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30711392-0729-4c77-bb54-94ae14135871",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae1643b-372b-4965-8dc3-533e5586fc15",
   "metadata": {},
   "source": [
    "Open the web UI of the [Mlflow Tracking Server](http://localhost:5001) again and view the runs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d528778-3b05-4493-944f-2a10dccca60b",
   "metadata": {},
   "source": [
    "### Part 2: Log artifacts\n",
    "As already mentioned, any data can be logged as artifacts for a run. In the following example, we want to save both the confusion matrix and the classification report as a file and then log them as artifacts. \n",
    "\n",
    "Please complete the code so that both files are logged as artifacts. Then execute the code from `Part 2` cell by cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32c96eb-c0fb-44ce-b202-d7f2f4dd5af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = mlflow.start_run(experiment_id=exp_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763d8787-4e60-4c2d-8964-1cfe0120c1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(classifier: XGBClassifier, input_test: pd.DataFrame) -> np.ndarray:\n",
    "    predictions = classifier.predict(input_test)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ef85c6-dd34-494f-9e28-46ff10e9d192",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predict(classifier=classifier, input_test=input_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30268d67-1614-4110-a6d6-6ca05a0014ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze(\n",
    "    target_test: pd.Series,\n",
    "    predictions: np.ndarray,\n",
    "    target_names: pd.Series,\n",
    "):\n",
    "    category_labels = target_names.cat.categories\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    ConfusionMatrixDisplay.from_predictions(\n",
    "        target_test, predictions, ax=ax, display_labels=category_labels\n",
    "    )\n",
    "    ax.tick_params(axis=\"x\", labelrotation=70, labelbottom=True)\n",
    "    fig.savefig(\"./data/confusion_materix.png\", pad_inches=20)\n",
    "    report = classification_report(target_test, predictions, output_dict=True)\n",
    "    df_classification_report = pd.DataFrame(report).transpose()\n",
    "    df_classification_report.to_csv(\"./data/classification_report.csv\")\n",
    "    mlflow.log_artifact(\"./data/classification_report.csv\")\n",
    "    mlflow.log_artifact(\"./data/confusion_materix.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b80504-6bab-4698-b3eb-db7442f421bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze(target_test=target_test, predictions=predictions, target_names=target_names)\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1643c55a-c5ac-4994-a9ed-7a1830902cde",
   "metadata": {},
   "source": [
    "Open the web UI of the [Mlflow Tracking Server](http://localhost:5001) again and view the runs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26249457-3c15-4d12-9fc5-3d5ac51c1915",
   "metadata": {},
   "source": [
    "### Part 3: Log datasets\n",
    "Last but not least, we want to log datasets for our runs. To do this, we first load the dataset, which is available as a CSV file, as a Pandas DataFrame. With MLflow, it is possible to create an MLflow dataset based on a DataFrame. The target can be specified as a parameter, in our case this is `genre`. Please create an MLflow dataset and log it as a dataset for a run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa51e82c-3cd2-4771-ab43-e4e2f1223e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = mlflow.start_run(experiment_id=exp_id)\n",
    "data = pd.read_csv(\"./data/genres_standardized.csv\", sep=\";\")\n",
    "dataset = mlflow.data.from_pandas(data, targets=\"genre\")\n",
    "mlflow.log_input(dataset)\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e0cc30-e20a-4327-b304-c90cf7d3f567",
   "metadata": {},
   "source": [
    "Open the web UI of the [Mlflow Tracking Server](http://localhost:5001) again and view the runs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8278acea-2faa-4982-9d94-05297ddab281",
   "metadata": {},
   "source": [
    "### Part 4: All together\n",
    "To complete this exercise, we would like to combine the different functions and implementations and run them as one big run where everything is logged. Please paste the code to log the dataset and classifier into the cell below, run the code and view the result via the [Mlflow Tracking Server](http://localhost:5001) web UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f63d45d-6f41-4f6b-a102-c3330f059729",
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run(experiment_id=exp_id):\n",
    "    data = pd.read_csv(\"./data/genres_standardized.csv\", sep=\";\")\n",
    "    dataset = mlflow.data.from_pandas(data, targets=\"genre\")\n",
    "    mlflow.log_input(dataset)\n",
    "    input_train, input_test, target_train, target_test, target_names = split_data()\n",
    "    classifier = train_classifier(input_train, target_train)\n",
    "    mlflow.xgboost.log_model(classifier, \"spotify_genre_classifier\")\n",
    "    predictions = predict(classifier=classifier, input_test=input_test)\n",
    "    analyze(target_test=target_test, predictions=predictions, target_names=target_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f179aacc-70ed-4f48-b8e0-b00d2d34a718",
   "metadata": {},
   "source": [
    "## 3) Combine Mlflow with Dagster\n",
    "We have prepared another notebook (`/notebooks/dagster/dagster_exercise_ops_job_mlflow.ipynb`) for you to use Dagster's MLflow integration to automatically create a MLflow run of the Dagster pipelines.\n",
    "To do this, you need to add `required_resource_keys={\"mlflow\"}` to each `op` decorator when mlflow is used for logging in the OP. This will ensure that the Dagster pipeline is only executed when an MLflow resource is available for the Dagster job. You do not need to create an experiment or start a run. This is done by Dagster and the MLflow integration. \n",
    "\n",
    "The entry `resource_defs={\"mlflow\": mlflow_tracking}` must be added to the `job` decorator of the `spotify_genre_classification` job. This makes MLflow available to the job as a resource and can be used during execution. Finally, add the `@end_mlflow_on_run_finished` decorator to the job. This will end the MLflow run as soon as the Dagster job is finished. **Save the notebook dagster_exercise_ops_job_mlflow.\n",
    "\n",
    "If you now open the [Dagster UI](http://localhost:3000), update the code location and open the Launchpad of the `spotify_genre_classification` job of the dagster mllfow code location, Dagster displays an error message that the configuration is incomplete. Let Dagster adjust the configuration. \n",
    "\n",
    "Adjust the configuration so that it looks like this: \n",
    "\n",
    "``` yaml\n",
    "ops:\n",
    "  analyze:\n",
    "    config:\n",
    "      confusion_matrix_path: ./data/confusion_materix.png\n",
    "      report_path: ./data/classification_report.csv\n",
    "  split_data:\n",
    "    config:\n",
    "      data_path: ./data/genres_standardized.csv\n",
    "      seperator: ;\n",
    "      target_column: genre\n",
    "      test_set_size: 0.2\n",
    "  train_classifier:\n",
    "    config:\n",
    "      gamma: 0\n",
    "      learning_rate: 0.1\n",
    "      max_depth: 10\n",
    "      min_child_weight: 1\n",
    "      number_of_estimators: 100\n",
    "      number_of_jobs: 4\n",
    "resources:\n",
    "  mlflow:\n",
    "    config:\n",
    "      experiment_name: Spotify genre classification mlflow\n",
    "      mlflow_tracking_uri: http://mlflow:5001\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
