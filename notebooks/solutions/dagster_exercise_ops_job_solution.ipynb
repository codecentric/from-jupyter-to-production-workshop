{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ef90d78-209d-4fb2-af61-0b062fc1c48b",
   "metadata": {},
   "source": [
    "## Data Pipeline Orchestration with Dagster Ops\n",
    "\n",
    "In this notebook we will get to know the basics of dagster Ops. Therefore, we will create a simple training pipeline to train an XGBoost classifier.\n",
    "\n",
    "Dagsters definition of Ops:\n",
    "> Ops are the core unit of computation in Dagster. The computational core of a software-defined asset is an op. \n",
    "> An individual op should perform relatively simple tasks, such as:\n",
    "> * Deriving a dataset from other datasets\n",
    "> * Executing a database query\n",
    "> * Initiating a Spark job in a remote cluster\n",
    "> * Querying an API and storing the result in a data warehouse\n",
    "> * Sending an email or Slack message\n",
    "\n",
    "Based on the data created in the dagster assets exercise, we want to derive training and test data, train the classifier, create a prediction for the test data, and finally create an analysis to determine how well the classifier performs on the test data. \n",
    "\n",
    "Therefore, we planned a small training pipeline wich will perform the following steps:\n",
    "\n",
    "1. Split data into subsets -> Create a training and test dataset (`split_data` OP)\n",
    "2. Train classifier -> Fit a XGBoost classifier (`train` OP)\n",
    "3. Create predicitons -> Use the classifier to create predictions for the test data (`predict` OP)\n",
    "4. Analyse predictions -> Create a confusion matrix and a classification report for the predictions (`analyze` OP)\n",
    "\n",
    "The code for these tasks is already provided. All you need to do is put their logic together in the form of a dagster op job.\n",
    "\n",
    "After the definitions are complete, we will have a look at the dagster UI and run the op job."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3391f63a-f5de-40dd-89ed-00f4c439c534",
   "metadata": {},
   "source": [
    "Here are the imports, we will need for the whole task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d64c230-5973-4cd5-95aa-867e53bc0464",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dagster import AssetKey, Config, Definitions, In, OpExecutionContext, Out, job, op\n",
    "from pydantic import Field\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d24b4f-07b3-4854-a6be-b9883548384b",
   "metadata": {},
   "source": [
    "## 1) Split data into subsets\n",
    " The function `split_data` contains the logic, with wich the subsets are generated.\n",
    " `SplitDataConfig` is a dagster config class which makes it possible to adjust the configuration of the op. You will see, that this config, can be modified via the dagster UI, without changing the underlying code.\n",
    " \n",
    " By adding the `op` decorator to the `split_data` function, you define it as a dagster op. Please add the decorator.\n",
    " We want to give some more information about the op. As you can see, the function generates a total of four output values. Please define these output values as `Out`s for the op. You can also do this with `out` parameter of the op decorator that is dictionary (e.g. `\"test_value\":Out()`).\n",
    " The output parameter should have the following naming to stay consisten with the following ops:\n",
    " `input_train`, `input_test`, `target_train`, `target_test`, `target_names`.\n",
    "\n",
    "It could also be that the separator (`sep`) used to read the CSV files changes over time. Please add the `separator` as an additional configuration parameter to the `SplitDataConfig` and use the parameter in the `split_data` function. The default should be `;`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f4214f-1201-4ab5-8b38-565497b14cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SplitDataConfig(Config):\n",
    "    data_path: str = Field(\n",
    "        description=\"File path of the input data\",\n",
    "        default=\"./data/genres_standardized.csv\",\n",
    "    )\n",
    "    target_column: str = Field(\n",
    "        description=\"Column name of the target column\", default=\"genre\"\n",
    "    )\n",
    "    test_set_size: float = Field(\n",
    "        description=\"Size of the test set in percentage\", default=0.2\n",
    "    )\n",
    "    seperator: str = Field(\n",
    "        description=\"Seperator that should be used to load the data as a DataFrame\",\n",
    "        default=\";\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ce933a-e223-4101-a76d-03de326b59a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@op(\n",
    "    out={\n",
    "        \"input_train\": Out(),\n",
    "        \"input_test\": Out(),\n",
    "        \"target_train\": Out(),\n",
    "        \"target_test\": Out(),\n",
    "        \"target_names\": Out(),\n",
    "    }\n",
    ")\n",
    "def split_data(\n",
    "    config: SplitDataConfig,\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame, pd.Series, pd.Series, pd.Series]:\n",
    "    data = pd.read_csv(config.data_path, sep=config.seperator)\n",
    "    columns = list(data.columns)\n",
    "    columns.remove(config.target_column)\n",
    "    data[config.target_column] = data[config.target_column].astype(\"category\")\n",
    "    data[\"target\"] = data[config.target_column].cat.codes\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        data[columns], data[\"target\"], test_size=config.test_set_size\n",
    "    )\n",
    "    return X_train, X_test, y_train, y_test, data[config.target_column]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa6e0c8-bbbd-4881-b06c-81d231af3aaa",
   "metadata": {},
   "source": [
    "## 2) Train classifier\n",
    "The `train_classifier` method uses the previously created subsets and the associated targets to create an XGBoost classifier. The config class `TrainConfig` is also used here to configure the `train_classifier` op. For a better understanding, please add the inputs (`In`) `input_train` and `target_train` to the `ins` parameter of the op decorator (e.g. `test_value:In()`) that is a dict. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b4fd09-726b-441e-b879-0babca9ddf99",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainConfig(Config):\n",
    "    number_of_estimators: int = Field(description=\"Number of boosting rounds\", default=1000)\n",
    "    learning_rate: float = Field(description=\"Boosting learning rate\", default=0.1)\n",
    "    max_depth: int = Field(description=\"Maximum tree depth for base learners\", default=8)\n",
    "    min_child_weight: float = Field(\n",
    "        description=\"Minimum sum of instance weight(hessian) needed in a child\", default=1\n",
    "    )\n",
    "    gamma: float = Field(\n",
    "        description=\"Minimum loss reduction required to make a further partition on a leaf node of the tree\",\n",
    "        default=0,\n",
    "    )\n",
    "    number_of_jobs: int = Field(\n",
    "        description=\"Number of parallel threads used to run xgboost\", default=4\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9672f32b-762d-47a2-b0ec-fca9cb12b0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "@op(ins={\"input_train\": In(), \"target_train\": In()}, out={\"classifier\": Out()})\n",
    "def train_classifier(\n",
    "    config: TrainConfig, input_train: pd.DataFrame, target_train: pd.Series\n",
    ") -> XGBClassifier:\n",
    "    model = XGBClassifier(\n",
    "        learning_rate=config.learning_rate,\n",
    "        n_estimators=config.number_of_estimators,\n",
    "        max_depth=config.max_depth,\n",
    "        min_child_weight=config.min_child_weight,\n",
    "        gamma=config.gamma,\n",
    "        n_jobs=config.number_of_jobs,\n",
    "    )\n",
    "    model.fit(input_train, target_train)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e2720d-2c50-4909-ad4c-1436dccfc622",
   "metadata": {},
   "source": [
    "## 3) Create predictions\n",
    "The classifier created in the `train_classifier` op is used in the `predict` function to create predictions for `input_test` from the `split_data` op. \n",
    "To make it clear to other users of the pipeline in the Dagster UI what exactly happens in this op, please add a docstring(`\"\"\" \"\"\"`) to the function. \n",
    "\n",
    "The docstring could look like this: \n",
    "\n",
    "`In this op, a XGBoost classifier is used to generate predictions for a test set from the Spotify genres dataset.`\n",
    "\n",
    "Once you have added the docstring, you can save this notebook. \n",
    "Open the [Dagster UI](http://localhost:3000). \n",
    "\n",
    "> **_NOTE:_** To ensure that the latest code is used, update the code location (Deployment -> `dagster_exercise_ops_jobs.py` -> Reload).\n",
    "\n",
    "Go to `Overview` in the main menu. There, click on Jobs and select the job `spotify_genre_classification`. The job graph opens. The op `prediction` should now have the docstring as a description.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ae8e03-102f-4824-a04f-bd79404242de",
   "metadata": {},
   "outputs": [],
   "source": [
    "@op(ins={\"classifier\": In(), \"input_test\": In()}, out={\"predictions\": Out()})\n",
    "def predict(classifier: XGBClassifier, input_test: pd.DataFrame) -> np.ndarray:\n",
    "    \"\"\"In this op, a XGBoost classifier is used to generate predictions for\n",
    "    a test set from the Spotify genres dataset.\"\"\"\n",
    "    predictions = classifier.predict(input_test)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb8bf20-51b8-43ca-8513-b06eb87ef5d2",
   "metadata": {},
   "source": [
    "## 4) Analyze predictions\n",
    "Finally, the generated predictions must be analyzed so that the classifier can be evaluated. The analysis is performed by the `analyze` function. \n",
    "In contrast to the other op's, this op has a parameter `context` of the type `OpExecutionContext`. It is possible to access values and functions of the execution of the op via this context. A logger (`context.log`) is also available via the context, which can be used to log something during the execution of the op. \n",
    "\n",
    "Please log the accuracy (`df_classification_report.loc[\"accuracy\"].mean()`) via the context logger. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0f81a9-b745-4248-ac9a-e2b62d080dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnalyzeConfig(Config):\n",
    "    confusion_matrix_path: str = Field(default=\"./data/confusion_materix.png\")\n",
    "    report_path: str = Field(default=\"./data/classification_report.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7a9329-91d3-4f3e-afc6-55a000e0db34",
   "metadata": {},
   "outputs": [],
   "source": [
    "@op(ins={\"target_test\": In(), \"predictions\": In(), \"target_names\": In()})\n",
    "def analyze(\n",
    "    context: OpExecutionContext,\n",
    "    config: AnalyzeConfig,\n",
    "    target_test: pd.Series,\n",
    "    predictions: np.ndarray,\n",
    "    target_names: pd.Series,\n",
    "):\n",
    "    target_test = np.asarray(target_test),\n",
    "    category_labels = target_names.cat.categories\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    ConfusionMatrixDisplay.from_predictions(\n",
    "        target_test, predictions, ax=ax, display_labels=category_labels\n",
    "    )\n",
    "    ax.tick_params(axis=\"x\", labelrotation=70, labelbottom=True)\n",
    "    fig.savefig(config.confusion_matrix_path, pad_inches=20)\n",
    "    report = classification_report(target_test, predictions, output_dict=True)\n",
    "    df_classification_report = pd.DataFrame(report).transpose()\n",
    "    df_classification_report.to_csv(config.report_path)\n",
    "    context.log.info(\"Accuracy:%s\", df_classification_report.loc[\"accuracy\"].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da12836-7720-4c83-8451-65f12ef39a89",
   "metadata": {},
   "source": [
    "## 5) Create an op job\n",
    "To define a job from the individual ops, the ops (functions) must be linked to each other via their return values. \n",
    "Create an op job that uses the previously created ops. \n",
    "\n",
    "> *Note*: The parameters `config` and `context` do not have to be set when calling the ops. dagster will do this for us later. \n",
    "\n",
    "First call the `split_data` op and save the return values in variables so that you know later which subsets and targets are behind which variables. As a reminder, the return value names of `split_data` are: `input_train`, `input_test`, `target_train`, `target_test`, `target_names`).\n",
    "\n",
    "Then call the method `train_classifier` and pass the parameters `input_train` and `target_train`. Save the return value (`classifier`) in a variable as well. \n",
    "\n",
    "Proceed in the same way with the ops `predict` and `analyze`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6702cef1-11f4-4ab6-ba9a-54cc06cbd8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "@job()\n",
    "def spotify_genre_classification():\n",
    "    input_train, input_test, target_train, target_test, target_names = split_data()\n",
    "    classifier = train_classifier(input_train=input_train, target_train=target_train)\n",
    "    predictions = predict(classifier=classifier, input_test=input_test)\n",
    "    analyze(target_test=target_test, predictions=predictions, target_names=target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774ad547-600a-48e5-9e26-9ade3f3004c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "defs = Definitions(jobs=[spotify_genre_classification])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598aace7-1e11-4f6d-9069-952517dcb99a",
   "metadata": {},
   "source": [
    "## 6) Start the job via the Dagster UI\n",
    "Open the [Dagster UI](http://localhost:3000)\n",
    "You will see the `Overview` page by default. Click on the `Jobs` tab and open the `spotify_genre_classification` job again. In addition to the `Overview` tab, there is also a `Launchpad` tab. Open the Launchpad. You should see something like that: \n",
    "\n",
    "<img src=\"./data/assets/dagster_ui_ops_job.png\" width=\"1000\" height=\"1000\">\n",
    "\n",
    "You can start the job using the `Launch Run` button in the bottom right-hand corner. Start the job. As soon as the job has run successfully, you should also see your logged accuracy relatively far down in the events. You can find the created confusion matrix and the other metrics here in the jupyter lab in the `data` folder. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac553c0-dd5a-4ecf-9bf1-d6d8b20836b0",
   "metadata": {},
   "source": [
    "## 7) Adjust train config \n",
    "The pipelines can be easily configured via the Dagster UI. \n",
    "\n",
    "First remove any parameter from the configuration (e.g. `seperator`). Then the 'Scaffold all default config' button will be activated. Click that button. The deleted entry is added again with the default value from the source code. \n",
    "\n",
    "Now edit the `TrainConfig` in the source code. \n",
    "Remove the default value for the parameter `number_of_estimators`. Save this notebook.\n",
    "\n",
    "In the Launchpad of the Dagster UI, you will find a small reload button to the right of the job title (`spotify_genre_classification`) with which you can update the code. Update the code and remove the `number_of_estimators` parameter from the Launchpad. \n",
    "\n",
    "<img src=\"./data/assets/dagster_ui_ops_job_missing_config.png\" width=\"600\" height=\"600\">\n",
    "\n",
    "You will notice that Dagster displays an error that a configuration entry is missing. Unlike a parameter with a default value, Dagster cannot execute the job without the `number_of_estimatirs` parameter. If you click on the `Scaffold missing config` button, dagster adds the values to the launchpad and initializes it with the value `0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d1bdfa-29da-4b6e-b3ed-48a43ae1bf65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
