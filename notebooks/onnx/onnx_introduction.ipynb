{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open Neural Network Exchange (ONNX)\n",
    "\n",
    "ONNX is an open format designed to facilitate the interoperability of deep learning frameworks. It provides a standardized way to represent deep learning models, enabling developers to move models between different frameworks seamlessly.\n",
    "\n",
    "In this tutorial, we want to see how we can train a model with scikit-learn and export it to an ONNX model, such that we could use somewhere else (maybe a mobile phone, an edge device, or just some container).\n",
    "\n",
    "### Scenario\n",
    "\n",
    "For this tutorial, we will use the popular loan prediction task. The [loan prediction dataset](https://www.kaggle.com/datasets/altruistdelhite04/loan-prediction-problem-dataset) contains features such as applicant demographics, income, credit history, and loan details. This dataset is widely used in machine learning tutorials for a classification task: will a loan be granted to the applicant? Controversely enough, it also serves as a showcase for responsible / ethical AI, since the historical loan data disadvantages women."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Preparation\n",
    "\n",
    "The data preprocessing and training is not part of the tutorial per se, but we want to be transparent, which preparatory steps have been performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# NOTE: although we load a file called `train.csv`, we will also use it for model validation.\n",
    "# we will use the `test.csv` later for the use with ONNX\n",
    "df = pd.read_csv('./data/train.csv')\n",
    "# lowercase columns for simplicity\n",
    "df.columns = map(str.lower, df.columns)\n",
    "# let's have a look at our data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loan_status is our target column\n",
    "y = df[\"loan_status\"]\n",
    "\n",
    "# drop the loan_status (target), loan_id (irrelevant for training) and gender (discriminatory)\n",
    "X = df.drop([\"loan_status\", \"loan_id\", \"gender\"], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DType-specific transformations\n",
    "\n",
    "We need to apply different types of pre-processing depending on our data type, so we distinguish first, which columns are numerical, and which ones categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.api.types import is_numeric_dtype\n",
    "\n",
    "categorical_features = []\n",
    "numerical_features = []\n",
    "\n",
    "for column_name, column in X.items():\n",
    "    if is_numeric_dtype(column):\n",
    "        numerical_features.append(column_name)\n",
    "    else:\n",
    "        categorical_features.append(column_name)\n",
    "\n",
    "print(f\"Numerical features: {numerical_features}\")\n",
    "print(f\"Categorical features: {categorical_features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train / Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    # for numerical features, we impute missing values with the mean.\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    # we standardize the value distribuction into a 0-1 range\n",
    "    ('scaler', MinMaxScaler())])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    # for categorical feature, we perform a OneHot-Encoding, since our model architecture - logistic regression - cannot handle categoricals natively\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('numeric', numeric_transformer, numerical_features),\n",
    "        ('categorical', categorical_transformer, categorical_features)])\n",
    "\n",
    "# our classifier is a logistic regression\n",
    "clf = LogisticRegression()\n",
    "\n",
    "# our model pipeline consistes of the set of preprocessors and the classifier\n",
    "# NOTE that this means, that preprocessing will automatically be also done during inference -> no need for a separate preprocessor\n",
    "pipeline = Pipeline(\n",
    "    steps=[('preprocessor', preprocessor), ('classifier', clf)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .score() returns the mean accuracy.\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert model to ONNX\n",
    "\n",
    "There are different Python libraries that can help convert machine learning artifacts to ONNX. `skl2onnx` is one of the most popular ones and - unsurprisingly - used to convert sklearn artifacts. Other libraries support Tensorflow, PyTorch, XGBoost and more. Of course, GenAI models are also supported.\n",
    "\n",
    "For `skl2onnx`, you can find a list of all supported sklearn objects in [this list](https://onnx.ai/sklearn-onnx/supported.html#l-converter-list). \n",
    "\n",
    "## Data Types\n",
    "\n",
    "ONNX specifications are optimized for numerical computation with tensors.  As such, we need to convert our pandas data types to ONNX tensor types.\n",
    "\n",
    "NOTE: ONNX is strongly typed and its definition does not support implicit cast. It is impossible to add two tensors or matrices with different types even if other languages does. Thatâ€™s why an explicit cast must be inserted in a graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skl2onnx.common.data_types import FloatTensorType, StringTensorType\n",
    "\n",
    "# annotate, which features need to correspond to which onnx type.\n",
    "# we set the shape of our input type to be [None, 1], which allows us to perform a batch prediction\n",
    "# as well.\n",
    "initial_types = \\\n",
    "    [(categorical, StringTensorType([None, 1])) for categorical in categorical_features] + \\\n",
    "    [(numerical, FloatTensorType([None, 1])) for numerical in numerical_features]\n",
    "initial_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skl2onnx import to_onnx\n",
    "\n",
    "# the actual conversion to an ONNX graph is done by `to_onnx()`, which needs the model and dtype information.\n",
    "model_onnx = to_onnx(pipeline, initial_types=initial_types)\n",
    "with open(\"model/loan_model.onnx\", \"wb\") as f:\n",
    "    f.write(model_onnx.SerializeToString())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test ONNX model\n",
    "\n",
    "Now that we have create an ONNX model, let's how we can infer predictions with it. For this demonstration, we will use the Python runtime. This is obviously unnecessarily complicated, since we could just use the sklearn model directly, and won't make use of hardware acceleration. Yet it allows use to test how we can use a model standardized by ONNX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# unfortunately, onnx does not support pandas dataframe out of the box. We need to convert our input into a python dict with\n",
    "# the structure { input_name: input_value }, where input_value is either python-native or a numpy array.\n",
    "input_data = {column: X_test[column].values for column in X_test.columns}\n",
    "for numeric in numerical_features:\n",
    "    input_data[numeric] = input_data[numeric].astype(np.float32).reshape((input_data[numeric].shape[0], 1))\n",
    "for categorical in categorical_features:\n",
    "    input_data[categorical] = input_data[categorical].reshape((input_data[categorical].shape[0], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now the features have the expected shape [None, 1]\n",
    "for name, feature in input_data.items():\n",
    "    print(name, feature.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import onnxruntime as rt\n",
    "\n",
    "# initialise the session. A session can be used repeatedly on an inference server/backend\n",
    "sess = rt.InferenceSession(\"model/loan_model.onnx\")\n",
    "\n",
    "# invoke the session with the test data\n",
    "# the first parameter - here set to None - defines the output name of the prediction. We don't care about that.\n",
    "pred_onnx = sess.run(None, input_data)\n",
    "pred_onnx[1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
