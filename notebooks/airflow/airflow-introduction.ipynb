{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Airflow Classification Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at some of the components that Airflow provides. Please note: Airflow DAGs are to be specified in `.py` files, not Jupyter notebooks. This notebook only serves the presentation purpose and can not be directly used in Airflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Directed Acyclic Graph (DAG)\n",
    "\n",
    "Each Airflow pipeline has to be a directed acyclic graph (i.e. no loops, clear task dependencies).\n",
    "We define a DAG in the following way:\n",
    "\n",
    "```python\n",
    "from airflow import DAG\n",
    "\n",
    "# the default args will be passed to all tasks belonging to the DAG\n",
    "default_args = {\n",
    "    'owner': 'airflow',\n",
    "    'start_date': datetime(2019, 8, 1),\n",
    "    ...\n",
    "}\n",
    "\n",
    "dag = DAG(\n",
    "    'keras2production-pipeline',\n",
    "    default_args=default_args,\n",
    "    schedule_interval=timedelta(days=1),\n",
    "    ...\n",
    ")\n",
    "\n",
    "```\n",
    "\n",
    "DAGs need to be placed into the folder `$AIRFLOW_HOME/dags`, where `$AIRFLOW_HOME` is usually set to `/usr/local/airflow`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Operators\n",
    "\n",
    "Each pipeline consists of steps, which are called operators. Operators execute some kind of task and finish when the task is done. There are also sensors, a special type of operators, which finish when a condition is met (i.e. another task has finished or a file exists). Don't worry so much about the difference between a normal operator and a sensor, just lookup what fits your needs the best in the well-documented API.\n",
    "\n",
    "This might be the most used operator: The `PythonOperator`.\n",
    "\n",
    "```python\n",
    "from airflow.operators.python_operator import PythonOperator\n",
    "\n",
    "def print_sth():\n",
    "    print(\"Our slides are still there!\")\n",
    "    \n",
    "python_task = PythonOperator(\n",
    "    task_id=\"print\",\n",
    "    python_callable=print_sth,\n",
    "    provide_context=False,\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "```\n",
    "\n",
    "This is an example of a sensor:\n",
    "\n",
    "```python\n",
    "from airflow.contrib.sensors.file_sensor import FileSensor\n",
    "\n",
    "fs_task = FileSensor(\n",
    "    task_id=\"lookup_file\",\n",
    "    filepath=\"/keras2production/slides/slides.pdf\",\n",
    "    dag=dag\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connecting Operators\n",
    "\n",
    "Airflow uses the bitshift operators `>>` and `<<` to connect pipeline steps together (you can also use `set_upstream()` or `set_downstream()`, but just don't...).\n",
    "\n",
    "`task1 >> task2` means: `task1` will be executed before `task2`, and vice versa for `<<`.\n",
    "\n",
    "**Tip**: Only use the `>>` to keep your code consistent and readable.\n",
    "\n",
    "Let's connect our operator and sensor that we defined above:\n",
    "\n",
    "```python\n",
    "fs_task >> python_task\n",
    "```\n",
    "\n",
    "That's it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XCom\n",
    "\n",
    "Ideally all of our operators and sensors are independent of each other, that is: No variables, states etc. should have to be communicated between them. This allows an easy exchange of tasks. Since we cannot always avoid communication between tasks, Airflow provides the XCom (_cross-communication_) system, which follows a simple push-pull idea.\n",
    "\n",
    "You can push a variable in an operator via `xcom_push()` or the `return` statement, and pull (i.e. get) the variable in an other task by `xcom_pull()`.\n",
    "\n",
    "This is an example, how two python operators could communicate with each other via XCom:\n",
    "\n",
    "```python\n",
    "def foo():\n",
    "    x = \"hello world\"\n",
    "    return x\n",
    "\n",
    "\n",
    "def bar(**context):\n",
    "    x = context[\"task_instance\"].xcom_pull(task_ids=\"task1\")\n",
    "    print(x)\n",
    "    \n",
    "    \n",
    "task1 = PythonOperator(\n",
    "    task_id=\"task1\",\n",
    "    python_callable=foo,\n",
    ")\n",
    "\n",
    "task2 = PythonOperator(\n",
    "    task_id=\"task2\",\n",
    "    python_callable=bar,\n",
    "    provide_context=True,\n",
    ")\n",
    "\n",
    "task1 >> task2\n",
    "```\n",
    "\n",
    "**Note**: The pulling task needs to be provided with context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The web interface\n",
    "\n",
    "To monitor and manage your pipelines, Airflow offers a web interface. You can find it today at `localhost:8080`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# There is so much more!\n",
    "\n",
    "Airflow has many concepts to offer, which allow a robust but flexible execution. Some highlights are:\n",
    "- Error and retry management\n",
    "- Different executors (Celery, Sequential, Local), allowing for parallel execution on multiple nodes\n",
    "- Integration with GCP, AWS, Azure, ...\n",
    "\n",
    "Check out the [documentation page](https://airflow.apache.org/) for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Airflow\n",
    "Taken from [Quick Start](https://airflow.apache.org/docs/stable/start.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting apache-airflow==1.8.1\n",
      "  Using cached apache-airflow-1.8.1.tar.gz (2.3 MB)\n",
      "\u001b[31m    ERROR: Command errored out with exit status 1:\n",
      "     command: /usr/local/envs/workshop/bin/python3.7 -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-install-ukd5v3c7/apache-airflow/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-install-ukd5v3c7/apache-airflow/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' egg_info --egg-base /tmp/pip-pip-egg-info-htyplxw1\n",
      "         cwd: /tmp/pip-install-ukd5v3c7/apache-airflow/\n",
      "    Complete output (6 lines):\n",
      "    Traceback (most recent call last):\n",
      "      File \"<string>\", line 1, in <module>\n",
      "      File \"/tmp/pip-install-ukd5v3c7/apache-airflow/setup.py\", line 112\n",
      "        async = [\n",
      "              ^\n",
      "    SyntaxError: invalid syntax\n",
      "    ----------------------------------------\u001b[0m\n",
      "\u001b[31mERROR: Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# install from pypi using pip\n",
    "!pip install apache-airflow==1.8.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# packages in environment at /usr/local/envs/workshop:\n",
      "#\n",
      "# Name                    Version                   Build  Channel\n",
      "werkzeug                  1.0.1              pyh9f0ad1d_0    conda-forge\n"
     ]
    }
   ],
   "source": [
    "# initialize the database\n",
    "!conda list werkzeug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ____________       _____________\n",
      " ____    |__( )_________  __/__  /________      __\n",
      "____  /| |_  /__  ___/_  /_ __  /_  __ \\_ | /| / /\n",
      "___  ___ |  / _  /   _  __/ _  / / /_/ /_ |/ |/ /\n",
      " _/_/  |_/_/  /_/    /_/    /_/  \\____/____/|__/\n",
      "[\u001b[34m2020-09-30 09:49:28,713\u001b[0m] {\u001b[34m__init__.py:\u001b[0m50} INFO\u001b[0m - Using executor \u001b[01mSequentialExecutor\u001b[22m\u001b[0m\n",
      "[\u001b[34m2020-09-30 09:49:28,713\u001b[0m] {\u001b[34mdagbag.py:\u001b[0m417} INFO\u001b[0m - Filling up the DagBag from \u001b[01m/root/airflow/dags\u001b[22m\u001b[0m\n",
      "/usr/local/envs/workshop/lib/python3.7/site-packages/airflow/models/dag.py:1342: PendingDeprecationWarning: The requested task could not be added to the DAG because a task with task_id create_tag_template_field_result is already in the DAG. Starting in Airflow 2.0, trying to overwrite a task will raise an exception.\n",
      "  category=PendingDeprecationWarning)\n",
      "Running the Gunicorn Server with:\n",
      "Workers: 4 sync\n",
      "Host: 0.0.0.0:8080\n",
      "Timeout: 120\n",
      "Logfiles: - -\n",
      "=================================================================            \n",
      "[2020-09-30 09:49:29 +0000] [3039] [INFO] Starting gunicorn 20.0.4\n",
      "[2020-09-30 09:49:29 +0000] [3039] [INFO] Listening at: http://0.0.0.0:8080 (3039)\n",
      "[2020-09-30 09:49:29 +0000] [3039] [INFO] Using worker: sync\n",
      "[2020-09-30 09:49:29 +0000] [3042] [INFO] Booting worker with pid: 3042\n",
      "[2020-09-30 09:49:29 +0000] [3043] [INFO] Booting worker with pid: 3043\n",
      "[2020-09-30 09:49:29 +0000] [3044] [INFO] Booting worker with pid: 3044\n",
      "[2020-09-30 09:49:29 +0000] [3045] [INFO] Booting worker with pid: 3045\n",
      "[\u001b[34m2020-09-30 09:49:30,105\u001b[0m] {\u001b[34m__init__.py:\u001b[0m50} INFO\u001b[0m - Using executor \u001b[01mSequentialExecutor\u001b[22m\u001b[0m\n",
      "[\u001b[34m2020-09-30 09:49:30,105\u001b[0m] {\u001b[34mdagbag.py:\u001b[0m417} INFO\u001b[0m - Filling up the DagBag from \u001b[01m/root/airflow/dags\u001b[22m\u001b[0m\n",
      "[\u001b[34m2020-09-30 09:49:30,109\u001b[0m] {\u001b[34m__init__.py:\u001b[0m50} INFO\u001b[0m - Using executor \u001b[01mSequentialExecutor\u001b[22m\u001b[0m\n",
      "[\u001b[34m2020-09-30 09:49:30,115\u001b[0m] {\u001b[34mdagbag.py:\u001b[0m417} INFO\u001b[0m - Filling up the DagBag from \u001b[01m/root/airflow/dags\u001b[22m\u001b[0m\n",
      "/usr/local/envs/workshop/lib/python3.7/site-packages/airflow/models/dag.py:1342: PendingDeprecationWarning: The requested task could not be added to the DAG because a task with task_id create_tag_template_field_result is already in the DAG. Starting in Airflow 2.0, trying to overwrite a task will raise an exception.\n",
      "  category=PendingDeprecationWarning)\n",
      "/usr/local/envs/workshop/lib/python3.7/site-packages/airflow/models/dag.py:1342: PendingDeprecationWarning: The requested task could not be added to the DAG because a task with task_id create_tag_template_field_result is already in the DAG. Starting in Airflow 2.0, trying to overwrite a task will raise an exception.\n",
      "  category=PendingDeprecationWarning)\n",
      "[\u001b[34m2020-09-30 09:49:30,175\u001b[0m] {\u001b[34m__init__.py:\u001b[0m50} INFO\u001b[0m - Using executor \u001b[01mSequentialExecutor\u001b[22m\u001b[0m\n",
      "[\u001b[34m2020-09-30 09:49:30,175\u001b[0m] {\u001b[34mdagbag.py:\u001b[0m417} INFO\u001b[0m - Filling up the DagBag from \u001b[01m/root/airflow/dags\u001b[22m\u001b[0m\n",
      "/usr/local/envs/workshop/lib/python3.7/site-packages/airflow/models/dag.py:1342: PendingDeprecationWarning: The requested task could not be added to the DAG because a task with task_id create_tag_template_field_result is already in the DAG. Starting in Airflow 2.0, trying to overwrite a task will raise an exception.\n",
      "  category=PendingDeprecationWarning)\n",
      "[\u001b[34m2020-09-30 09:49:30,288\u001b[0m] {\u001b[34m__init__.py:\u001b[0m50} INFO\u001b[0m - Using executor \u001b[01mSequentialExecutor\u001b[22m\u001b[0m\n",
      "[\u001b[34m2020-09-30 09:49:30,289\u001b[0m] {\u001b[34mdagbag.py:\u001b[0m417} INFO\u001b[0m - Filling up the DagBag from \u001b[01m/root/airflow/dags\u001b[22m\u001b[0m\n",
      "/usr/local/envs/workshop/lib/python3.7/site-packages/airflow/models/dag.py:1342: PendingDeprecationWarning: The requested task could not be added to the DAG because a task with task_id create_tag_template_field_result is already in the DAG. Starting in Airflow 2.0, trying to overwrite a task will raise an exception.\n",
      "  category=PendingDeprecationWarning)\n",
      "[2020-09-30 09:49:58 +0000] [3039] [INFO] Handling signal: ttin\n",
      "[2020-09-30 09:49:58 +0000] [3066] [INFO] Booting worker with pid: 3066\n",
      "[\u001b[34m2020-09-30 09:49:59,269\u001b[0m] {\u001b[34m__init__.py:\u001b[0m50} INFO\u001b[0m - Using executor \u001b[01mSequentialExecutor\u001b[22m\u001b[0m\n",
      "[\u001b[34m2020-09-30 09:49:59,269\u001b[0m] {\u001b[34mdagbag.py:\u001b[0m417} INFO\u001b[0m - Filling up the DagBag from \u001b[01m/root/airflow/dags\u001b[22m\u001b[0m\n",
      "/usr/local/envs/workshop/lib/python3.7/site-packages/airflow/models/dag.py:1342: PendingDeprecationWarning: The requested task could not be added to the DAG because a task with task_id create_tag_template_field_result is already in the DAG. Starting in Airflow 2.0, trying to overwrite a task will raise an exception.\n",
      "  category=PendingDeprecationWarning)\n",
      "[2020-09-30 09:50:00 +0000] [3039] [INFO] Handling signal: ttou\n",
      "[2020-09-30 09:50:00 +0000] [3042] [INFO] Worker exiting (pid: 3042)\n",
      "\u001b[0m[2020-09-30 09:50:29 +0000] [3039] [INFO] Handling signal: ttin\n",
      "[2020-09-30 09:50:29 +0000] [3072] [INFO] Booting worker with pid: 3072\n",
      "[\u001b[34m2020-09-30 09:50:29,752\u001b[0m] {\u001b[34m__init__.py:\u001b[0m50} INFO\u001b[0m - Using executor \u001b[01mSequentialExecutor\u001b[22m\u001b[0m\n",
      "[\u001b[34m2020-09-30 09:50:29,753\u001b[0m] {\u001b[34mdagbag.py:\u001b[0m417} INFO\u001b[0m - Filling up the DagBag from \u001b[01m/root/airflow/dags\u001b[22m\u001b[0m\n",
      "/usr/local/envs/workshop/lib/python3.7/site-packages/airflow/models/dag.py:1342: PendingDeprecationWarning: The requested task could not be added to the DAG because a task with task_id create_tag_template_field_result is already in the DAG. Starting in Airflow 2.0, trying to overwrite a task will raise an exception.\n",
      "  category=PendingDeprecationWarning)\n",
      "[2020-09-30 09:50:30 +0000] [3039] [INFO] Handling signal: ttou\n",
      "[2020-09-30 09:50:30 +0000] [3043] [INFO] Worker exiting (pid: 3043)\n",
      "\u001b[0m[2020-09-30 09:50:59 +0000] [3039] [INFO] Handling signal: ttin\n",
      "[2020-09-30 09:50:59 +0000] [3086] [INFO] Booting worker with pid: 3086\n",
      "[\u001b[34m2020-09-30 09:51:00,214\u001b[0m] {\u001b[34m__init__.py:\u001b[0m50} INFO\u001b[0m - Using executor \u001b[01mSequentialExecutor\u001b[22m\u001b[0m\n",
      "[\u001b[34m2020-09-30 09:51:00,215\u001b[0m] {\u001b[34mdagbag.py:\u001b[0m417} INFO\u001b[0m - Filling up the DagBag from \u001b[01m/root/airflow/dags\u001b[22m\u001b[0m\n",
      "/usr/local/envs/workshop/lib/python3.7/site-packages/airflow/models/dag.py:1342: PendingDeprecationWarning: The requested task could not be added to the DAG because a task with task_id create_tag_template_field_result is already in the DAG. Starting in Airflow 2.0, trying to overwrite a task will raise an exception.\n",
      "  category=PendingDeprecationWarning)\n",
      "[2020-09-30 09:51:00 +0000] [3039] [INFO] Handling signal: ttou\n",
      "[2020-09-30 09:51:00 +0000] [3044] [INFO] Worker exiting (pid: 3044)\n",
      "\u001b[0m[2020-09-30 09:51:30 +0000] [3039] [INFO] Handling signal: ttin\n",
      "[2020-09-30 09:51:30 +0000] [3092] [INFO] Booting worker with pid: 3092\n",
      "[\u001b[34m2020-09-30 09:51:30,736\u001b[0m] {\u001b[34m__init__.py:\u001b[0m50} INFO\u001b[0m - Using executor \u001b[01mSequentialExecutor\u001b[22m\u001b[0m\n",
      "[\u001b[34m2020-09-30 09:51:30,736\u001b[0m] {\u001b[34mdagbag.py:\u001b[0m417} INFO\u001b[0m - Filling up the DagBag from \u001b[01m/root/airflow/dags\u001b[22m\u001b[0m\n",
      "/usr/local/envs/workshop/lib/python3.7/site-packages/airflow/models/dag.py:1342: PendingDeprecationWarning: The requested task could not be added to the DAG because a task with task_id create_tag_template_field_result is already in the DAG. Starting in Airflow 2.0, trying to overwrite a task will raise an exception.\n",
      "  category=PendingDeprecationWarning)\n",
      "[2020-09-30 09:51:31 +0000] [3039] [INFO] Handling signal: ttou\n",
      "[2020-09-30 09:51:31 +0000] [3045] [INFO] Worker exiting (pid: 3045)\n",
      "\u001b[0m[2020-09-30 09:52:00 +0000] [3039] [INFO] Handling signal: ttin\n",
      "[2020-09-30 09:52:00 +0000] [3136] [INFO] Booting worker with pid: 3136\n",
      "[\u001b[34m2020-09-30 09:52:01,177\u001b[0m] {\u001b[34m__init__.py:\u001b[0m50} INFO\u001b[0m - Using executor \u001b[01mSequentialExecutor\u001b[22m\u001b[0m\n",
      "[\u001b[34m2020-09-30 09:52:01,177\u001b[0m] {\u001b[34mdagbag.py:\u001b[0m417} INFO\u001b[0m - Filling up the DagBag from \u001b[01m/root/airflow/dags\u001b[22m\u001b[0m\n",
      "/usr/local/envs/workshop/lib/python3.7/site-packages/airflow/models/dag.py:1342: PendingDeprecationWarning: The requested task could not be added to the DAG because a task with task_id create_tag_template_field_result is already in the DAG. Starting in Airflow 2.0, trying to overwrite a task will raise an exception.\n",
      "  category=PendingDeprecationWarning)\n",
      "[2020-09-30 09:52:01 +0000] [3039] [INFO] Handling signal: ttou\n",
      "[2020-09-30 09:52:01 +0000] [3066] [INFO] Worker exiting (pid: 3066)\n",
      "\u001b[0m[2020-09-30 09:52:31 +0000] [3039] [INFO] Handling signal: ttin\n",
      "[2020-09-30 09:52:31 +0000] [3158] [INFO] Booting worker with pid: 3158\n",
      "[\u001b[34m2020-09-30 09:52:31,882\u001b[0m] {\u001b[34m__init__.py:\u001b[0m50} INFO\u001b[0m - Using executor \u001b[01mSequentialExecutor\u001b[22m\u001b[0m\n",
      "[\u001b[34m2020-09-30 09:52:31,882\u001b[0m] {\u001b[34mdagbag.py:\u001b[0m417} INFO\u001b[0m - Filling up the DagBag from \u001b[01m/root/airflow/dags\u001b[22m\u001b[0m\n",
      "/usr/local/envs/workshop/lib/python3.7/site-packages/airflow/models/dag.py:1342: PendingDeprecationWarning: The requested task could not be added to the DAG because a task with task_id create_tag_template_field_result is already in the DAG. Starting in Airflow 2.0, trying to overwrite a task will raise an exception.\n",
      "  category=PendingDeprecationWarning)\n",
      "[2020-09-30 09:52:32 +0000] [3039] [INFO] Handling signal: ttou\n",
      "[2020-09-30 09:52:32 +0000] [3072] [INFO] Worker exiting (pid: 3072)\n",
      "\u001b[0m[2020-09-30 09:53:01 +0000] [3039] [INFO] Handling signal: ttin\n",
      "[2020-09-30 09:53:01 +0000] [3164] [INFO] Booting worker with pid: 3164\n",
      "[\u001b[34m2020-09-30 09:53:02,393\u001b[0m] {\u001b[34m__init__.py:\u001b[0m50} INFO\u001b[0m - Using executor \u001b[01mSequentialExecutor\u001b[22m\u001b[0m\n",
      "[\u001b[34m2020-09-30 09:53:02,393\u001b[0m] {\u001b[34mdagbag.py:\u001b[0m417} INFO\u001b[0m - Filling up the DagBag from \u001b[01m/root/airflow/dags\u001b[22m\u001b[0m\n",
      "/usr/local/envs/workshop/lib/python3.7/site-packages/airflow/models/dag.py:1342: PendingDeprecationWarning: The requested task could not be added to the DAG because a task with task_id create_tag_template_field_result is already in the DAG. Starting in Airflow 2.0, trying to overwrite a task will raise an exception.\n",
      "  category=PendingDeprecationWarning)\n",
      "[2020-09-30 09:53:03 +0000] [3039] [INFO] Handling signal: ttou\n",
      "[2020-09-30 09:53:03 +0000] [3086] [INFO] Worker exiting (pid: 3086)\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# start the web server, default port is 8080\n",
    "!airflow webserver -p 8080"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
