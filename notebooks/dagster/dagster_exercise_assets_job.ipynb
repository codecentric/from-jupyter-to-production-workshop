{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85da63223cedbce7",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Data Pipeline Orchestration with Dagster Assets\n",
    "\n",
    "In this notebook we will get to know the basics of dagster Assets. Therefore, we will create a simple data pipeline for preprocessing.\n",
    "\n",
    "**Dagsters definition of Asset and SDA:**\n",
    "> An asset is an object in persistent storage, such as a table, file, or persisted machine learning model. A software-defined asset is a description, in code, of an asset that should exist and how to produce and update that asset.\n",
    "\n",
    "In our use case we have a look at a song data set from Spotify. We want to clean the data, find and remove duplicates and prepare it, so we could use it to predict a songs music genre.\n",
    "\n",
    "Therefore, we planned a small preprocessing pipeline wich will perform the following steps:\n",
    "\n",
    "1. Load song data -> `song_data` Asset\n",
    "2. Remove unnecessary data -> `data_cleaned` Asset\n",
    "3. Extract Duplicates -> `duplicates` Asset\n",
    "4. Deduplicate data -> `data_deduplicated` Asset\n",
    "5. Perform one-hot-encoding on the `key` column -> `data_encoded` Asset\n",
    "6. Standardize columns with a wide value range and save as a csv -> `data_standardized` Asset\n",
    "\n",
    "The code for these tasks is already provided. All you need to do is put their logic together in the form of a dagster asset graph.\n",
    "\n",
    "After the definitions are complete, we will have a look at the dagster UI and materialize the assets.\n",
    "In the end, we will define Asset jobs as an alternative way to materialize a set of assets and run them in the UI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efee12a10ae3012e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Here are the imports, we will need for the whole task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b40f59188c17b6",
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import pandas as pd\n",
    "from dagster import asset, Config, Output, define_asset_job, Definitions, AssetSelection\n",
    "from pandas import DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7f13ed9c51cd7e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 1) Load song data\n",
    "\n",
    " The function `load_data` contains the logic, with wich the asset is generated.\n",
    " `DataConfig` is a dagster config class which makes it possible to adjust the materialization of the asset. For example, by providing a different `input_file` path. You will see, that this config, can be modified via the dagster UI, without changing the underlying code. This means, that by providing an asset config, the SDA is very flexible.\n",
    " \n",
    "By adding the `asset` decorator to the `load_data` function, you define it as a dagster asset and could already use and see it via the dagster UI.\n",
    "We want to give some more information about the asset. As `load_data` is not a good name for a data instance, rename the asset to `song_data` and give it a proper description. Both can be done by modifying the 'asset' decorator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6438e643bfadfe1",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class DataConfig(Config):\n",
    "    input_file: str = \"data/genres_v2.csv\"\n",
    "    url: str = \"https://www.kaggle.com/datasets/mrmorj/dataset-of-songs-in-spotify/data\"\n",
    "\n",
    "\n",
    "# ...\n",
    "def load_data(config: DataConfig) -> DataFrame:\n",
    "    return pd.read_csv(config.input_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3a414f060aef3c",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 2) Remove unnecessary data\n",
    "\n",
    "Let's have a look at the next asset. The name of this asset is ok, but we still have to define it as a dagster asset and there is no description, yet. Additionally, we have to change the name of the input dataframe to be the same as the name of our asset from task 1. Dagster will then connect the two assets.\n",
    "Modify the code cell to add the missing information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b554d724792e4dc",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ...\n",
    "def data_cleaned(song_data: pd.DataFrame):\n",
    "    return song_data.drop(\n",
    "        [\n",
    "            \"type\",\n",
    "            \"id\",\n",
    "            \"uri\",\n",
    "            \"track_href\",\n",
    "            \"analysis_url\",\n",
    "            \"song_name\",\n",
    "            \"Unnamed: 0\",\n",
    "            \"title\",\n",
    "        ],\n",
    "        axis=1,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2580ec45e7506028",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 3) Extract Duplicates\n",
    "\n",
    "For this task we have to define the function as an asset and give it a proper description.\n",
    "Taking into account, that our preprocessing pipeline or input data could change, it would be interesting to know, how many duplicated entries are in our dataset. By adding a monitoring, we would know if the number of duplicated entries changes.\n",
    "\n",
    "For dagster assets you can track such information by saving additional `metadata` to your output. `dagster.Output` provides a simple way to do so. Instead of directly returning `df`, return an `Output` object, with `df` as value and a dictionary as `metadata`. This metadata dict should contain one entry representing the number of rows and another representing the number of columns.\n",
    "\n",
    "As the metadata values are numeric, dagster will automatically display them as time-based graphs in the dagster UI. So, if you materialize the asset multiple times, you will be able to nicely see how the values changed over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8913537ac1e0c3",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ...\n",
    "def duplicates(data_cleaned: pd.DataFrame):\n",
    "    df = data_cleaned[data_cleaned.duplicated()]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5ec668ac1a3435",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 4) Deduplicate Data & 5) Perform one-hot-encoding on the 'key' column\n",
    "\n",
    "For the next two ops, do the same as in 3) for the `deduplicated_data` asset.\n",
    "Define them as Assets, add a proper description and add the metadata containing number of rows and columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0c334a92149850",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ...\n",
    "def data_deduplicated(data_cleaned: pd.DataFrame):\n",
    "    df = data_cleaned.drop_duplicates(keep=\"first\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9ec188df5bdcb9",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ...\n",
    "def data_encoded(data_deduplicated: pd.DataFrame):\n",
    "    df = pd.get_dummies(data_deduplicated, columns=[\"key\"], prefix=\"key\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb33ebd17cb57c76",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 6) Standardize columns with a wide value range and save as a csv\n",
    "\n",
    "This is the last asset you need to define for the preprocessing pipeline. On top of the basic tasks, which you've done in tasks 3, 4 and 5, define a new metadata entry. This time it is not a numeric value, but the list of standardized columns.\n",
    "\n",
    "You may have noticed, that the columns, which are standardized, are hardcoded. Let's change that by providing a config for this asset. The `StandardizationConfig` should be an instance of `dagster.Config` with a single attribute `columns_to_standardize`. Insert the config as an input of `data_standardized` and replace the `columns_to_standardize` variable with the according config attribute.\n",
    "\n",
    "If you need guidance, have a look at the first asset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5187bcc5697f7f3",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ...\n",
    "\n",
    "\n",
    "# ...\n",
    "def data_standardized(data_encoded: pd.DataFrame):\n",
    "    pd.set_option(\"display.max_columns\", 500)\n",
    "    data_encoded.describe()\n",
    "    columns_to_standardize: List[str] = [\"duration_ms\", \"tempo\"]\n",
    "    for col in columns_to_standardize:\n",
    "        data_encoded[col] = (data_encoded[col] - data_encoded[col].min()) / (\n",
    "            data_encoded[col].max() - data_encoded[col].min()\n",
    "        )\n",
    "    data_encoded.to_csv(\"data/genres_standardized.csv\", sep=\";\", index=False)\n",
    "    return data_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab633f485003bb1e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 7) Have a look at the Dagster UI\n",
    "\n",
    "We now have all of our assets defined, combined and configured. Let's have a look at the [Dagster UI](http://localhost:3000).\n",
    "\n",
    "The UI was started automatically, when the docker container of this workshop was started.\n",
    "\n",
    "Navigating to the `Assets` tab, you can see all the assets you defined, where they are located and if they were materialized yet. \n",
    "\n",
    "*Don't see anything?* You may need to reload your code locations. Navigate to the `Deployments` tab and reload the `dagster_exercise_assets_job.py`.\n",
    "\n",
    "Select the `song_data` asset and materialize it. You notice, that a run is started, which performs the materialization. If the run was successfully completed, the status of the asset will change to `Materialized`.\n",
    "\n",
    "Navigate to the `global asset lineage`, via the link in the upper right corner. Here you can see how the assets are connected with each other. Materialize all assets, to see if your definitions are correct.\n",
    "\n",
    "After materialization, you can get more information about the assets by clicking on them. Have a look at `data_deduplicated` and the generated metadata plots. Optionally, redo the materialization and see how new metadata values are added. Dagster will also notice, that the next assets are now outdated, as `data_deduplicated` was updated separately.\n",
    "\n",
    "Optionally, have a look at an asset in the asset catalog. It stores all information about the asset, like metadata, materialization timestamp, old materializations, heritage, code version and data version.\n",
    "\n",
    "Congratulation! You successfully defined and combined your first assets to a working preprocessing pipeline!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59eb1f3b743fc217",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 8) Define Asset jobs\n",
    "\n",
    "Materializing via the dagster UI is simple, but if you need to materialize a set of assets repeatedly, there is a better way - Dagster Asset Jobs.\n",
    "\n",
    "With an asset job, you combine the materialization of multiple assets in one job. This is similar to selecting multiple assets in the `Asset` tab or `global asset lineage` and click materialize. But, you define it in your code, which gives you the opportunity to determine sets of workflows.\n",
    "\n",
    "An asset job can be defined using dagsters `define_asset_job` function. You need to set the `name` parameter and may give a selection of assets you want to materialize in this job (string or list of strings). If you don't give a selection, all assets in the same file will be selected.\n",
    "\n",
    "Define an `all_assets_job`, which materializes all assets.\n",
    "Define an `get_duplicates_job`, which only materializes the `duplicates` asset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16da5a8dd4bc7746",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# uncommend the following lines and complete them\n",
    "# get_all_assets_job = define_asset_job(...)\n",
    "# get_duplicates_job = define_asset_job(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0064b01cc65ca4",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Unlike assets, jobs are not automatically displayed in the dagster UI. You need to specify all jobs (and assets) you want to see in a dagster Definitions using its `assets` and `jobs` parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d4fcbc6d9be871",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# uncommend the following lines and complete them\n",
    "# defs = Definitions(\n",
    "#     assets=...,\n",
    "#     jobs=...,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e151738cdf4256",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Have a look at the dagster UI again. \n",
    "\n",
    "*Don't see the jobs?* You may need to reload your deployment to see the changes in the `Deployment` tab.\n",
    "\n",
    "Navigate to `Overview` -> `Jobs` and see your defined jobs listed there. Click on one of the jobs and see the asset lineage of all assets, which are materialized with this job. Run the materialization.\n",
    "\n",
    "Congratulations! You also learned how to define asset jobs!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456d366e0910a579",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 9) Optional Task: Define an asset group\n",
    "\n",
    "As the number of assets can grow in a complex data processing pipeline, dagster offers to define asset groups. They can be used to keep your assets organized.\n",
    "\n",
    "Not all of the above assets are needed to get a cleaned data set. The `duplicates` asset is informative, but not explicitly needed. We want to have the option to save time and only materialize the necessary assets. Therefore, we will group all but `duplicates` in an asset group called `datapreprocessing`.\n",
    "\n",
    "Add the `group_name` parameter to all assets but `duplicates` and see how the asset lineage visualization changed.\n",
    "\n",
    "Define an asset job only for the assets in the asset group. Instead of listing each asset separatly, use `AssetSelection.groups(\"datapreprocessing\")` to define the selection. Don't forget to add it to your Definitions.\n",
    "\n",
    "Have a look at the dagster UI for the new job and materialize its assets.\n",
    "\n",
    "Hooray! You now gained a very good insight into dagster assets and its functionality!  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd18adc4bcf9576",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
